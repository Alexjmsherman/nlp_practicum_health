{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase (collocation) Detection Solution\n",
    "\n",
    "###### Author: Alex Sherman | alsherman@deloitte.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agenda\n",
    "1. SpaCy POS phrases\n",
    "2. Gensim Phrases and Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from collections import defaultdict\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from IPython.core.display import display, HTML\n",
    "from configparser import ConfigParser, ExtendedInterpolation\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration for data, acronyms, and gensim paths\n",
    "config = ConfigParser(interpolation=ExtendedInterpolation())\n",
    "config.read('../../config.ini')\n",
    "\n",
    "DB_PATH = config['DATABASES']['PROJECT_DB_PATH']\n",
    "MATCHED_TEXT_PATH = config['NLP']['MATCHED_TEXT_PATH']\n",
    "CLEANED_TEXT_PATH = config['NLP']['CLEANED_TEXT_PATH']\n",
    "GENSIM_DICTIONARY_PATH = config['NLP']['GENSIM_DICTIONARY_PATH']\n",
    "GENSIM_CORPUS_PATH = config['NLP']['GENSIM_CORPUS_PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count # of documents\n",
    "engine = create_engine(DB_PATH)\n",
    "pd.read_sql(\"SELECT COUNT(*) FROM pubmed \", con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>acid base diagrams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>immunising against receptors for antigen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>immune complexes in rheumatic disease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>loss of hla antigens associated with hormonal state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>hypocalcaemia after thyroidectomy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                                 text\n",
       "0      0                                   acid base diagrams\n",
       "1      1             immunising against receptors for antigen\n",
       "2      2                immune complexes in rheumatic disease\n",
       "3      3  loss of hla antigens associated with hormonal state\n",
       "4      4                    hypocalcaemia after thyroidectomy"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_sql(\"SELECT * FROM pubmed LIMIT 500000\", con=engine)\n",
    "\n",
    "# filter to relevant sections\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store section matches in list\n",
    "text = [text for text in df['text']]\n",
    "\n",
    "# review first sentence of a section match\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load spacy nlp model\n",
    "# use 'en' if you don't have the lg model\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### collect sentences using SpaCy matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_phrase_model_sents(matcher, doc, i, matches):\n",
    "    # identify matching spans (phrases)\n",
    "    match_id, start, end = matches[i]\n",
    "    # get sentence with matched term\n",
    "    sent = doc[start:end].sent.text\n",
    "    \n",
    "    # collect matching (cleaned) sents\n",
    "    matched_sents.append(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### match sentences\n",
    "\n",
    "WARNING VERY SLOW!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 43min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# remove false statement below to run code\n",
    "if 1 == 0:\n",
    "    # match sentences with the word disease or disorder\n",
    "    matched_sents = []\n",
    "    pattern = [[{'LOWER': 'disease'}], [{'LOWER': 'disorder'}]]\n",
    "\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    # use *patterns to add more than one pattern at once\n",
    "    matcher.add('disease_disorder', collect_phrase_model_sents, *pattern)\n",
    "\n",
    "    for doc in nlp.pipe(text, disable=['tagger','ner']):    \n",
    "        matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 3086 \n",
      "\n",
      "Example Match:\n",
      "immune complexes in rheumatic disease\n"
     ]
    }
   ],
   "source": [
    "print('Number of matches: {} \\n'.format(len(matched_sents)))\n",
    "\n",
    "print('Example Match:')\n",
    "print(matched_sents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export matched text to avoid repeating processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\alsherman\\\\Desktop\\\\NLP\\\\nlp_practicum_health_baseline\\\\raw_data/cleaned_text/matched_text.txt'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view path to matched text\n",
    "MATCHED_TEXT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below to write the matched text to a .txt file for later use \n",
    "\n",
    "#with open(MATCHED_TEXT_PATH, 'w') as f:\n",
    "#    for line in matched_sents:\n",
    "#        line += '\\n'\n",
    "#        line = line.encode('ascii', errors='ignore').decode('ascii') \n",
    "#        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read matched text\n",
    "with open(MATCHED_TEXT_PATH, 'r') as f:\n",
    "    matched_sents_full = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>immune complexes in rheumatic disease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gluten and lymphocytes in coeliac disease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this activity correlated best with the severity and duration of the disease rather than with gam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>evidence exists that they control some of the peripheral manifestations of the disease including...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the finding of additional substitute cf antigens for the hrvl agent may have implications in the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                             sentences\n",
       "0                                                                immune complexes in rheumatic disease\n",
       "1                                                            gluten and lymphocytes in coeliac disease\n",
       "2  this activity correlated best with the severity and duration of the disease rather than with gam...\n",
       "3  evidence exists that they control some of the peripheral manifestations of the disease including...\n",
       "4  the finding of additional substitute cf antigens for the hrvl agent may have implications in the..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store all matched sentences in a dataframe\n",
    "matches_df = pd.DataFrame(matched_sents_full, columns=['sentences'])\n",
    "\n",
    "# remove duplicates\n",
    "matches_df = matches_df.drop_duplicates()\n",
    "\n",
    "# recreate matched_sents (since it takes so long to create on its own)\n",
    "matched_sents = [sent[0].split() for sent in matches_df.values]\n",
    "\n",
    "# view matches\n",
    "matches_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use SpaCy part of speech (POS) to create phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this activity correlated best with the severity and duration of the disease rather than with gamma globulin or total protein concentrations'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine the matched sentence tokens and parse it with SpaCy\n",
    "text = ' '.join(matched_sents[2])\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Determine which NLP components can be disabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_pos(doc, n_tokens=5):\n",
    "    \"\"\" print SpaCy POS information about each token in a provided document \"\"\"\n",
    "    print('{:15} | {:10} | {:10} | {:30}'.format('TOKEN','POS','DEP_','LEFTS'))\n",
    "    for token in doc[0:n_tokens]:\n",
    "        print('{:15} | {:10} | {:10} | {:30}'.format(\n",
    "            token.text, token.head.pos_,token.dep_, str([t.text for t in token.lefts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN           | POS        | DEP_       | LEFTS                         \n",
      "this            | NOUN       | det        | []                            \n",
      "activity        | VERB       | nsubj      | ['this']                      \n",
      "correlated      | VERB       | ROOT       | ['activity']                  \n",
      "best            | VERB       | advmod     | []                            \n",
      "with            | VERB       | prep       | []                            \n"
     ]
    }
   ],
   "source": [
    "# observe which part of speech (pos) attributes are disabled by named entity recognition (ner)\n",
    "pos_doc = nlp(text, disable=['ner'])\n",
    "view_pos(pos_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN           | POS        | DEP_       | LEFTS                         \n",
      "this            | DET        |            | []                            \n",
      "activity        | NOUN       |            | []                            \n",
      "correlated      | VERB       |            | []                            \n",
      "best            | ADV        |            | []                            \n",
      "with            | ADP        |            | []                            \n"
     ]
    }
   ],
   "source": [
    "# observe which part of speech (pos) attributes are disabled by parser\n",
    "pos_doc = nlp(text, disable=['ner','parser'])\n",
    "view_pos(pos_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN           | POS        | DEP_       | LEFTS                         \n",
      "this            |            | det        | []                            \n",
      "activity        |            | nsubj      | ['this']                      \n",
      "correlated      |            | ROOT       | ['activity']                  \n",
      "best            |            | advmod     | []                            \n",
      "with            |            | prep       | []                            \n"
     ]
    }
   ],
   "source": [
    "# observe which part of speech (pos) attributes are disabled by tagger\n",
    "pos_doc = nlp(text, disable=['ner','tagger'])\n",
    "view_pos(pos_doc, n_tokens=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'direct object'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use explain to define any token.dep_ attributes\n",
    "spacy.explain('dobj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=https://spacy.io/api/annotation#dependency-parsing width=1000 height=400></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependency_parsing_labels_url = 'https://spacy.io/api/annotation#dependency-parsing'\n",
    "iframe = '<iframe src={} width=1000 height=400></iframe>'.format(dependency_parsing_labels_url)\n",
    "HTML(iframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract phrases by identifying tokens describing an object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add stop words to SpaCy\n",
    "# this enables the .is_stop attribute with common stop words\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "for word in STOP_WORDS:\n",
    "    lex = nlp.vocab[word]\n",
    "    lex.is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pos_phrases(doc):\n",
    "\n",
    "    phrases = [] \n",
    "\n",
    "    doc = nlp(doc, disable=['ner','tagger'])\n",
    "    for token in doc:\n",
    "        # find any objects (e.g. direct objects )\n",
    "        if 'obj' in token.dep_:\n",
    "            token_text = token.lemma_.lower()\n",
    "            \n",
    "            # find any dependent terms to the left of (preceeding) the object\n",
    "            # ignore dependent terms that are not stop words\n",
    "            for left_term in (t.text for t in token.lefts if t.is_stop is False):\n",
    "                # combine the dependent term and object, separated by an underscore\n",
    "                # e.g. travel agency ==> travel_agency\n",
    "                phrase = '{}_{}'.format(left_term,token_text)\n",
    "                phrases.append(phrase)\n",
    "    \n",
    "    # convert list of distinct phrases into a sentence\n",
    "    return ' '.join(set(phrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>immune complexes in rheumatic disease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gluten and lymphocytes in coeliac disease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this activity correlated best with the severity and duration of the disease rather than with gam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>evidence exists that they control some of the peripheral manifestations of the disease including...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the finding of additional substitute cf antigens for the hrvl agent may have implications in the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                             sentences\n",
       "0                                                                immune complexes in rheumatic disease\n",
       "1                                                            gluten and lymphocytes in coeliac disease\n",
       "2  this activity correlated best with the severity and duration of the disease rather than with gam...\n",
       "3  evidence exists that they control some of the peripheral manifestations of the disease including...\n",
       "4  the finding of additional substitute cf antigens for the hrvl agent may have implications in the..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review data\n",
    "matches_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rheumatic_disease\n",
      "coeliac_disease\n",
      "gamma_globulin\n",
      "thyrotoxic_process underlying_process peripheral_manifestation cardiac_output palpitations_tachycardia\n",
      "hrvl_agent cf_antigen additional_antigen human_disease\n",
      "Wall time: 771 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for sent in matched_sents_full[0:5]:\n",
    "    print(create_pos_phrases(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 473 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                                                                                      rheumatic_disease\n",
       "1                                                                                        coeliac_disease\n",
       "2                                                                                         gamma_globulin\n",
       "3    thyrotoxic_process underlying_process peripheral_manifestation cardiac_output palpitations_tachy...\n",
       "4                                                 hrvl_agent cf_antigen additional_antigen human_disease\n",
       "Name: sentences, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# apply the custom function to every element in the dataframe\n",
    "matches_df[0:5].sentences.apply(create_pos_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pandas Apply\n",
    "\n",
    "apply is an efficient and fast approach to 'apply' a function to every element in a row. applymap does the same to every element in the entire dataframe (e.g. convert all ints to floats)\n",
    "\n",
    "Example: https://chrisalbon.com/python/data_wrangling/pandas_apply_operations_to_dataframes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1  col2\n",
       "0     0     3\n",
       "1     1     4\n",
       "2     2     5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a small dataframe with example data\n",
    "test_df = pd.DataFrame({'col1':range(0,3),'col2':range(3,6)})\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.0\n",
       "1    1.0\n",
       "2    2.0\n",
       "Name: col1, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply a built-in function to each element in a column\n",
    "test_df['col1'].apply(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5\n",
       "1    6\n",
       "2    7\n",
       "Name: col1, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply a custom function to every element in a column\n",
    "def add_five(row):\n",
    "    return row + 5\n",
    "\n",
    "test_df['col1'].apply(add_five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5\n",
       "1    6\n",
       "2    7\n",
       "Name: col1, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply an annonomous function to every element in a column\n",
    "test_df['col1'].apply(lambda x: x+5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1  col2\n",
       "0   0.0   3.0\n",
       "1   1.0   4.0\n",
       "2   2.0   5.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply a built-in function to every element in a dataframe \n",
    "test_df.applymap(float)  # applymap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocations\n",
    "\n",
    "\"A collocation is an expression consisting of two or more words that\n",
    "correspond to some conventional way of saying things. Or in the words\n",
    "of Firth (1957: 181): “Collocations of a given word are statements of the\n",
    "habitual or customary places of that word.” Collocations include noun\n",
    "phrases like strong tea and weapons of mass destruction, phrasal verbs like\n",
    "to make up, and other stock phrases like the rich and powerful. Particularly\n",
    "interesting are the subtle and not-easily-explainable patterns of word usage\n",
    "that native speakers all know: why we say a stiff breeze but not a stiff wind\n",
    "(while either a strong breeze or a strong wind is okay), or why we speak of\n",
    "broad daylight (but not bright daylight or narrow darkness)\n",
    "\n",
    "\n",
    "\n",
    "There are actually different definitions of the notion of collocation. Some\n",
    "authors in the computational and statistical literature define a collocation\n",
    "as two or more consecutive words with a special behavior, for example\n",
    "Choueka (1988):\n",
    "[A collocation is defined as] a sequence of two or more consecutive\n",
    "words, that has characteristics of a syntactic and semantic\n",
    "unit, and whose exact and unambiguous meaning or connotation\n",
    "cannot be derived directly from the meaning or connotation of its\n",
    "components. In most linguistically oriented research, a phrase\n",
    "can be a collocation even if it is not consecutive (as in the example knock\n",
    ". . . door). The following criteria are typical of linguistic treatments of collocations:\n",
    "\n",
    "**Non-compositionality**: The meaning of a collocation is not a straightforward\n",
    "composition of the meanings of its parts. Either the meaning\n",
    "is completely different from the free combination (as in the case of idioms\n",
    "like kick the bucket) or there is a connotation or added element of\n",
    "meaning that cannot be predicted from the parts. For example, white\n",
    "wine, white hair and white woman all refer to slightly different colors, so\n",
    "we can regard them as collocations. \n",
    "\n",
    "**Non-substitutability**: We cannot substitute near-synonyms for the\n",
    "components of a colloction. For example, we can’t say yellow wine\n",
    "instead of white wine even though yellow is as good a description of the\n",
    "color of white wine as white is (it is kind of a yellowish white).\n",
    "\n",
    "**Non-modifiability**: Many collocations cannot be freely modified with\n",
    "additional lexical material or through grammatical transformations.\n",
    "This is especially true for frozen expressions like idioms. For example,\n",
    "we can’t modify frog in to get a frog in one’s throat into to get an ugly\n",
    "frog in one’s throat although usually nouns like frog can be modified by\n",
    "adjectives like ugly. Similarly, going from singular to plural can make\n",
    "an idiom ill-formed, for example in people as poor as church mice.\"\n",
    "\n",
    "SOURCE: https://nlp.stanford.edu/fsnlp/promo/colloc.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Create a function that returns a window of size n over a given sentence. \n",
    "\n",
    "For the sentence **'rather than pay the fee'** return the following if the window is n=3:\n",
    "- ['rather', 'than', 'pay'],\n",
    "- ['than','pay','the']\n",
    "- ['pay', 'the','fee']\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['immune', 'complexes', 'in', 'rheumatic', 'disease']\n"
     ]
    }
   ],
   "source": [
    "# example sentence\n",
    "sent = ' '.join(matches_df['sentences'][0:1]).split()\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['immune', 'complexes', 'in'],\n",
       " ['complexes', 'in', 'rheumatic'],\n",
       " ['in', 'rheumatic', 'disease']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_sentence_windows(sentence, n=3):\n",
    "    \"create a sliding window over the n terms in a list of terms\"\n",
    "        \n",
    "    # create a window on the first n terms by slicing the sentence into the first n terms\n",
    "    window = sentence[0:n]\n",
    "    \n",
    "    # create a list to store all windows\n",
    "    # add the first window that was created above\n",
    "    sentence_windows = [window]\n",
    "\n",
    "    # iterate through the rest of the terms of the sentence\n",
    "    # e.g. if n=3, then create a new window with terms 2 to 4\n",
    "    for term in sentence[n:]:\n",
    "        # remove the first terms of the window and add the next term from the sentence\n",
    "        window = window[1:] + [term]\n",
    "        # add the updated window to the master list\n",
    "        sentence_windows.append(window)\n",
    "\n",
    "    return sentence_windows\n",
    "\n",
    "# execute the function\n",
    "sentence_window = create_sentence_windows(sent, n=3)\n",
    "# view the first few results\n",
    "sentence_window[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['immune', 'complexes', 'in'],\n",
       " ['complexes', 'in', 'rheumatic'],\n",
       " ['in', 'rheumatic', 'disease'],\n",
       " ['gluten', 'and', 'lymphocytes'],\n",
       " ['and', 'lymphocytes', 'in']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute the function for all sentences\n",
    "\n",
    "# create a list to store all windows\n",
    "sentence_window = []\n",
    "\n",
    "for sent in matches_df['sentences']:\n",
    "    # convert the sentence string into a list of terms\n",
    "    sent = sent.split()\n",
    "    \n",
    "    # create the sentence windows and append to the sentence_windows list\n",
    "    windows = create_sentence_windows(sent, n=3)\n",
    "    \n",
    "    # add each window to the sentence_window list\n",
    "    # iterate through windows to make each item in sentence window a window, not a list of windows\n",
    "    for window in windows:\n",
    "        sentence_window.append(window)\n",
    "\n",
    "# view the first five results\n",
    "sentence_window[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('heart_disease', 223),\n",
       " ('hodgkin_disease', 213),\n",
       " ('disease_virus', 168),\n",
       " ('whipple_disease', 152),\n",
       " ('liver_disease', 129),\n",
       " ('marek_disease', 107),\n",
       " ('coronary_heart', 98),\n",
       " ('vascular_disease', 90),\n",
       " ('mouth_disease', 89),\n",
       " ('crohn_disease', 86),\n",
       " ('graves_disease', 78),\n",
       " ('chronic_liver', 74),\n",
       " ('coronary_disease', 71),\n",
       " ('chronic_disease', 71),\n",
       " ('malignant_disease', 69),\n",
       " ('coeliac_disease', 69),\n",
       " ('ischaemic_heart', 64),\n",
       " ('parkinson_disease', 57),\n",
       " ('pulmonary_vascular', 52),\n",
       " ('pulmonary_disease', 51)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "\n",
    "# create a defaultdict to keep track of common phrases\n",
    "window_count = defaultdict(int)\n",
    "\n",
    "for sent in sentence_window:\n",
    "    # remove stop words\n",
    "    sentence = [term for term in sent if term not in STOP_WORDS]\n",
    "    \n",
    "    # create a combination of terms\n",
    "    # e.g. (rather, than, pay) --> (rather,than), (than,pay), (rather,pay)\n",
    "    for combo in combinations(sentence, 2):\n",
    "        # convert the tuple to a term\n",
    "        # e.g. (rather, than) --> 'rather_than'\n",
    "        phrase = '_'.join(combo)\n",
    "        \n",
    "        # increment the count for the term each time it appears to identify the most common terms\n",
    "        window_count[phrase] += 1\n",
    "\n",
    "# sort to view the most common terms\n",
    "# the key (lambda x: x[1]) sorts by the count\n",
    "sorted(window_count.items(), key=lambda x: x[1], reverse=True)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase (collocation) Detection\n",
    "\n",
    "Phrase modeling is another approach to learning combinations of tokens that together represent meaningful multi-word concepts. We can develop phrase models by looping over the the words in our reviews and looking for words that co-occur (i.e., appear one after another) together much more frequently than you would expect them to by random chance. The formula our phrase models will use to determine whether two tokens $A$ and $B$ constitute a phrase is:\n",
    "\n",
    "$$\\frac{count(A\\ B) - count_{min}}{count(A) * count(B)} > threshold$$\n",
    "\n",
    "- $count(A\\ B)$ is the number of times the tokens $A\\ B$ appear in the corpus in order\n",
    "- $count_{min}$ is a user-defined parameter to ensure that accepted phrases occur a minimum number of times\n",
    "- $count(A)$ is the number of times token $A$ appears in the corpus\n",
    "- $count(B)$ is the number of times token $B$ appears in the corpus\n",
    "- $threshold$ is a user-defined parameter to control how strong of a relationship between two tokens the model requires before accepting them as a phrase\n",
    "\n",
    "Once our phrase model has been trained on our corpus, we can apply it to new text. When our model encounters two tokens in new text that identifies as a phrase, it will merge the two into a single new token.\n",
    "\n",
    "Phrase modeling is superficially similar to named entity detection in that you would expect named entities to become phrases in the model (so new york would become new_york). But you would also expect multi-word expressions that represent common concepts, but aren't specifically named entities (such as happy hour) to also become phrases in the model.\n",
    "\n",
    "We turn to the indispensible gensim library to help us with phrase modeling — the Phrases class in particular.\n",
    "\n",
    "SOURCE: \n",
    "- https://github.com/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb\n",
    "- https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scikit-learn API for Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['immune', 'complexes', 'in', 'rheumatic', 'disease'], ['gluten', 'and', 'lymphocytes', 'in', 'coeliac', 'disease'], ['this', 'activity', 'correlated', 'best', 'with', 'the', 'severity', 'and', 'duration', 'of', 'the', 'disease', 'rather', 'than', 'with', 'gamma', 'globulin', 'or', 'total', 'protein', 'concentrations'], ['evidence', 'exists', 'that', 'they', 'control', 'some', 'of', 'the', 'peripheral', 'manifestations', 'of', 'the', 'disease', 'including', 'nervousness', 'palpitations', 'tachycardia', 'increased', 'cardiac', 'output', 'and', 'tremor', 'but', 'they', 'do', 'not', 'appear', 'to', 'affect', 'the', 'underlying', 'thyrotoxic', 'process', 'itself'], ['the', 'finding', 'of', 'additional', 'substitute', 'cf', 'antigens', 'for', 'the', 'hrvl', 'agent', 'may', 'have', 'implications', 'in', 'the', 'against', 'human', 'disease'], ['most', 'of', 'them', 'were', 'found', 'in', 'patients', 'with', 'digestive', 'tract', 'disease', 'essentially', 'colonic', 'cancer'], ['latency', 'in', 'herpesvirus', 'hominis', 'its', 'relationship', 'to', 'oncogenesis', 'and', 'recurrent', 'disease'], ['sixteen', 'of', 'patients', 'treated', 'with', 'intent', 'of', 'cure', 'are', 'considered', 'well', 'with', 'no', 'evidence', 'of', 'disease'], ['the', 'remaining', 'cases', 'were', 'considered', 'failures', 'all', 'died', 'except', 'one', 'living', 'with', 'disease'], ['six', 'of', 'twelve', 'cases', 'per', 'cent', 'survived', 'three', 'years', 'and', 'of', 'cases', 'per', 'cent', 'survived', 'for', 'one', 'year', 'free', 'of', 'disease']]\n"
     ]
    }
   ],
   "source": [
    "#sklearn_phrases.transform(matched_sents)\n",
    "print(matched_sents[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhrasesTransformer(delimiter=b'_', max_vocab_size=40000000, min_count=5,\n",
       "          progress_per=10000, scoring='default', threshold=5)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.sklearn_api.phrases import PhrasesTransformer\n",
    "\n",
    "sklearn_phrases = PhrasesTransformer(min_count=5, threshold=5)\n",
    "sklearn_phrases.fit(matched_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alsherman\\AppData\\Local\\Continuum\\anaconda3\\envs\\mlguild\\lib\\site-packages\\gensim\\models\\phrases.py:486: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lymph_node', 'were_treated', 'pick_disease', 'pathogenesis_of', 'with_chronic', 'eight_patients', 'was_used', 'alzheimer_disease', 'characterised_by', 'who_are', 'indicate_that', 'method_for', 'progression_of', 'fine_structure', 'risk_factors', 'rheumatoid_arthritis', 'inoculated_with', 'with_severe', 'an_outbreak', 'phytanic_acid', 'primary_biliary', 'acute_phase', 'long_term', 'showed_that', 'evidence_that', 'peripheral_blood', 'stage_iii', 'changes_in', 'characterized_by', 'resistance_to', 'whipple_disease', 'who_had', 'leukemia_virus', 'healthy_controls', 'forms_of', 'due_to', 'beta_microglobulin', 'presence_of', 'chronic_liver', 'coeliac_disease', 'greater_than', 'multiple_sclerosis', 'evidence_for', 'epstein_barr', 'radiation_therapy', 'density_lipoprotein', 'nature_of', 'chronic_granulomatous', 'treated_with', 'isolated_from', 'findings_suggest', 'sickle_cell', 'parkinson_disease', 'by_means', 'identical_to', 'congenital_heart', 'be_considered', 'immunized_with', 'observed_in', 'vaccine_against', 'huntington_disease', 'incidence_of', 'and_mouth', 'electron_microscopic', 'seems_to', 'may_be', 'breast_cancer', 'differ_from', 'but_not', 'was_shown', 'was_significantly', 'complete_remission', 'biliary_cirrhosis', 'whom_had', 'involved_in', 'periodontal_disease', 'findings_are', 'systemic_lupus', 'differential_diagnosis', 'as_an', 'history_of', 'paget_disease', 'patients_who', 'protection_against', 'concluded_that', 'is_discussed', 'gamma_globulin', 'do_not', 'liver_disease', 'disease_states', 'test_for', 'chronic_active', 'seven_patients', 'virus_infection', 'non_malignant', 'course_of', 'alpha_globulin', 'susceptibility_to', 'correlate_with', 'marek_disease', 'sera_from', 'migration_inhibition', 'beta_msh', 'correlation_between', 'not_only', 'more_severe', 'one_patient', 'suffering_from', 'compared_to', 'to_have', 'antithyroid_drugs', 'hbv_infection', 'central_nervous', 'crohn_disease', 'cell_lines', 'pulmonary_arterial', 'shown_to', 'evaluation_of', 'were_found', 'cellular_immunity', 'surface_antigen', 'author_transl', 'herpes_simplex', 'was_seen', 'used_to', 'lupus_erythematosus', 'lymph_nodes', 'it_was', 'obstructive_pulmonary', 'these_findings', 'to_determine', 'associated_with', 'alcoholic_liver', 'von_willebrand', 'were_seen', 'risk_for', 'also_be', 'except_for', 'results_indicate', 'patients_with', 'bowel_disease', 'mouth_disease', 'hypothesis_that', 'did_not', 'relation_to', 'suggesting_that', 'connective_tissue', 'prevalence_of', 'seen_in', 'is_described', 'had_been', 'cardiovascular_disease', 'remain_alive', 'as_compared', 'all_patients', 'is_reported', 'up_to', 'than_years', 'wilson_disease', 'the_pathogenesis', 'raised_serum', 'normal_subjects', 'were_investigated', 'autoimmune_thyroiditis', 'carried_out', 'hepatitis_virus', 'prepared_from', 'ulcerative_colitis', 'rheumatic_heart', 'immune_response', 'there_was', 'be_useful', 'drug_therapy', 'similar_to', 'can_be', 'are_described', 'ischaemic_heart', 'use_of', 'is_suggested', 'seem_to', 'are_discussed', 'artery_disease', 'to_prevent', 'was_demonstrated', 'were_obtained', 'clinical_manifestations', 'coronary_heart', 'must_be', 'as_well', 'normal_controls', 'peripheral_vascular', 'serum_levels', 'suggests_that', 'less_than', 'from_patients', 'responsible_for', 'induced_by', 'beh_et', 'were_studied', 'metastatic_disease', 'lipoprotein_cholesterol', 'guinea_pigs', 'tangier_disease', 'effect_of', 'screening_test', 'obstructive_lung', 'rather_than', 'lead_to', 'an_additional', 'to_be', 'coronary_artery', 'malignant_disease', 'year_old', 'antibodies_to', 'obtained_from', 'high_incidence', 'with_non', 'was_diagnosed', 'it_has', 'alkaline_phosphatase', 'significantly_higher', 'other_than', 'prenatal_diagnosis', 'to_investigate', 'with_crohn', 'but_also', 'non_hodgkin', 'pulmonary_vascular', 'family_history', 'an_important', 'was_not', 'hodgkin_disease', 'infectious_bursal', 'diagnostic_value', 'should_be', 'association_with', 'in_vitro', 'addison_disease', 'free_survival', 'immune_complexes', 'related_to', 'serum_afp', 'graft_versus', 'transcobalamin_ii', 'was_observed', 'alpha_fetoprotein', 'niemann_pick', 'patients_without', 'patient_with', 'alpha_antitrypsin', 'model_for', 'vascular_disease', 'motor_unit', 'could_be', 'immune_complex', 'an_increase', 'these_observations', 'specimens_from', 'results_suggest', 'refsum_disease', 'studies_on', 'these_results', 'resistant_to', 'the_presence', 'such_as', 'were_examined', 'cytomegalic_inclusion', 'with_hodgkin', 'chronic_obstructive', 'respiratory_disease', 'at_risk', 'aujeszky_disease', 'foot_and', 'no_evidence', 'derived_from', 'based_on', 'in_vivo', 'the_same', 'degree_of', 'molecular_weight', 'diagnosis_of', 'graves_disease', 'control_subjects', 'relationship_between', 'had_no', 'advanced_pulmonary', 'found_to', 'skin_tests', 'heart_disease', 'antibody_to', 'were_detected', 'cell_mediated', 'have_been', 'indicates_that', 'depends_on', 'ventricular_septal', 'newcastle_disease', 'autoimmune_disease', 'cushing_disease', 'experimental_allergic', 'at_different', 'who_were', 'basic_protein', 'electron_microscopy', 'might_be', 'more_than', 'ischemic_heart', 'detection_of', 'according_to', 'advanced_hodgkin', 'bone_marrow', 'has_been', 'development_of', 'clinical_course', 'left_ventricular', 'great_arteries', 'it_is', 'there_is', 'healthy_volunteers', 'the_authors', 'serum_alpha', 'gaucher_disease', 'stages_of', 'combination_chemotherapy', 'in_whom', 'in_order', 'myocardial_infarction', 'nervous_system', 'was_performed', 'swine_vesicular', 'this_disorder', 'legionnaires_disease', 'consistent_with', 'was_measured', 'high_density', 'children_with', 'number_of', 'inflammatory_bowel', 'with_graves', 'the_first', 'was_studied', 'per_cent', 'mixed_connective', 'five_patients', 'appear_to', 'relationship_to', 'months_after', 'without_evidence', 'biopsy_specimens', 'acute_leukaemia', 'caused_by', 'lymphocytes_from', 'consecutive_patients', 'from_normal', 'lymphoblastoid_cell', 'no_correlation', 'lung_disease', 'follow_up', 'suggest_that', 'nephrotic_syndrome', 'to_detect', 'absence_of', 'severity_of', 'small_intestine', 'than_those', 'the_possibility', 'response_to', 'these_data', 'duration_of', 'compared_with', 'is_not', 'density_lipoproteins', 'no_significant', 'able_to', 'suggested_that', 'showed_no', 'infected_with', 'against_marek', 'at_least', 'failed_to', 'was_found', 'antithyroid_drug', 'appears_to', 'effects_of', 'renal_tubular', 'were_determined', 'evidence_of', 'in_children', 'trophoblastic_disease', 'does_not'}\n"
     ]
    }
   ],
   "source": [
    "# review phrase matches\n",
    "phrases = []\n",
    "for terms in sklearn_phrases.transform(matched_sents):\n",
    "    for term in terms:\n",
    "        if term.count('_') >= 1:\n",
    "            phrases.append(term)\n",
    "\n",
    "print(set(phrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of stop words\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "common_terms = list(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**common_terms:** optional list of “stop words” that won’t affect frequency count of expressions containing them.\n",
    "- The common_terms parameter add a way to give special treatment to common terms (aka stop words) such that their presence between two words won’t prevent bigram detection. It allows to detect expressions like “bank of america” or “eye of the beholder”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gensim API\n",
    "A more complex API, though it is faster and has better integration with other gensim components (e.g. Phraser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.phrases.Phrases at 0x25df4213710>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases = Phrases(\n",
    "      matched_sents\n",
    "    , common_terms=common_terms\n",
    "    , min_count=5\n",
    "    , threshold=5\n",
    "    , scoring='default'\n",
    ")\n",
    "\n",
    "phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrases Params\n",
    "\n",
    "- **scoring:** specifies how potential phrases are scored for comparison to the threshold setting. scoring can be set with either a string that refers to a built-in scoring function, or with a function with the expected parameter names. Two built-in scoring functions are available by setting scoring to a string:\n",
    "\n",
    "    - ‘default’: from “Efficient Estimaton of Word Representations in Vector Space” by Mikolov, et. al.: \n",
    "    \n",
    "$$\\frac{count(AB) - count_{min}}{count(A) * count(B)} * N > threshold$$\n",
    "    \n",
    "\n",
    "    - where N is the total vocabulary size.\n",
    "    - Thus, it is easier to exceed the threshold when the two words occur together often or when the two words are rare (i.e. small product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.phrases.Phraser at 0x25dc84de278>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram = Phraser(phrases)\n",
    "\n",
    "bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The phrases object still contains all the source text in memory. A gensim Phraser will remove this extra data to become smaller and somewhat faster than using the full Phrases model. To determine what data to remove, the Phraser ues the  results of the source model’s min_count, threshold, and scoring settings. (You can tamper with those & create a new Phraser to try other values.)\n",
    "\n",
    "SOURCE: https://radimrehurek.com/gensim/models/phrases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_phrases(phraser, text_stream, num_underscores=2):\n",
    "    \"\"\" identify phrases from a text stream by searching for terms that\n",
    "        are separated by underscores and include at least num_underscores\n",
    "    \"\"\"\n",
    "    \n",
    "    phrases = []\n",
    "    for terms in phraser[text_stream]:\n",
    "        for term in terms:\n",
    "            if term.count('_') >= num_underscores:\n",
    "                phrases.append(term)\n",
    "    print(set(phrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'virus_of_aujeszky', 'patients_with_crohn', 'transposition_of_the_great', 'report_of_case', 'case_of_whipple', 'patient_with_hodgkin', 'outbreaks_of_respiratory', 'patients_with_graves', 'foot_and_mouth'}\n"
     ]
    }
   ],
   "source": [
    "print_phrases(bigram, matched_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tri-gram phrase model\n",
    "\n",
    "We can place the text from the first phrase model into another Phrases object to create n-term phrase models. We can repear this process multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acute_phase_of_the_disease', 'cases_of_graves_disease', 'patient_with_whipple_disease', 'syndrome_and_one_patient', 'cent_of_the_patients', 'changes_in_the_functional', 'manifestation_of_the_disease', 'outbreaks_of_respiratory_disease', 'originating_from_marek_disease', 'cases_of_legionnaires_disease', 'activity_of_the_disease', 'disease_report_of_case', 'disease_had_raised_serum', 'infants_with_congenital_heart', 'crohn_disease_and_in_normal_controls', 'thirty_eight_of_these_patients', 'hepatitis_to_chronic_liver', 'disease_and_in_particular', 'vitro_and_in_vivo', 'core_of_marek_disease', 'aged_from_to_years', 'patient_with_hodgkin_disease', 'related_to_the_activity', 'lymphocytes_in_hodgkin_disease', 'carcinoma_of_the_breast', 'course_of_the_disease', 'risk_for_coronary_heart', 'seven_patients_with_advanced_hodgkin', 'high_density_lipoprotein_cholesterol', 'strains_of_marek_disease', 'patients_with_whipple_disease', 'helpful_in_the_diagnosis', 'observations_on_outbreaks_of_respiratory', 'arrhythmias_in_coronary_heart', 'carcinoma_of_the_lung', 'half_of_the_patients', 'patients_with_parkinson_disease', 'pathogenesis_of_the_disease', 'leading_to_the_diagnosis', 'biopsy_specimens_from_patients', 'patients_with_other_types', 'patients_with_ulcerative_colitis', 'rheumatoid_arthritis_rheumatic_heart', 'course_of_antithyroid_drugs', 'mithramycin_in_paget_disease', 'virus_of_aujeszky_disease', 'form_of_the_disease', 'risk_factors_in_coronary_heart', 'risk_factors_for_coronary_heart', 'patients_with_hodgkin_disease', 'cases_of_chronic_liver', 'beginning_of_the_disease', 'considered_in_the_differential_diagnosis', 'pathogenesis_of_coeliac_disease', 'patients_with_transposition_of_the_great', 'findings_in_whipple_disease', 'deaths_from_coronary_heart', 'stage_of_the_disease', 'cases_of_whipple_disease', 'mucins_in_crohn_disease', 'patients_with_inflammatory_bowel', 'hyperthyroidism_of_graves_disease', 'correlation_between_serum_afp', 'similar_to_those_seen', 'cases_of_refsum_disease', 'matter_of_the_brain', 'discussed_in_the_light', 'release_of_foot_and_mouth', 'factor_in_the_pathogenesis', 'clinical_manifestations_of_the_disease', 'patients_with_crohn_disease', 'untreated_patients_with_graves', 'strains_of_newcastle_disease', 'reaction_of_the_synovial', 'treatment_of_chronic_obstructive', 'protection_against_marek_disease', 'prevention_of_alcoholic_liver', 'glucocorticoids_who_had_similar', 'associated_with_hodgkin_disease', 'duration_of_the_disease', 'severity_of_the_disease', 'chicks_against_marek_disease', 'patients_with_coronary_artery', 'marek_disease_and_turkey', 'acute_leukaemia_in_remission', 'alpha_fetoprotein_in_liver_disease', 'disease_may_be_difficult', 'patients_with_graves_disease', 'role_in_the_pathogenesis', 'recurrent_or_metastatic_disease', 'patients_with_autoimmune_thyroiditis', 'hand_foot_and_mouth', 'children_with_coeliac_disease', 'period_of_the_disease', 'patients_with_non_malignant', 'responses_in_huntington_disease', 'sign_of_the_disease', 'adenocarcinoma_of_the_prostate', 'occur_in_this_disease', 'vaccine_against_aujeszky_disease', 'stages_of_cushing_disease', 'complete_transposition_of_the_great', 'features_of_the_disease', 'patients_with_chronic_granulomatous', 'treatment_of_advanced_hodgkin', 'related_to_the_serum', 'whipple_disease_is_described', 'carcinoma_of_the_apocrine', 'nature_of_the_disease', 'phase_of_the_disease', 'isoenzymes_in_heart_disease', 'agent_of_legionnaires_disease', 'forms_of_the_disease', 'phases_of_the_disease', 'attenuated_virus_of_aujeszky', 'resection_of_the_prostate', 'series_of_the_foot_and_mouth', 'patients_with_cushing_disease', 'significantly_higher_in_the_patients', 'involvement_in_whipple_disease', 'advanced_pulmonary_vascular_disease', 'infected_with_the_virus', 'surgery_and_or_radiation', 'prevalence_of_hbv_infection', 'chronic_active_liver_disease', 'strains_of_infectious_bursal', 'nervous_system_by_malignant', 'progression_of_the_disease', 'paget_disease_of_the_nipple', 'aspects_of_whipple_disease', 'derived_from_marek_disease', 'regression_of_the_disease', 'observed_in_graves_disease', 'patients_with_peripheral_vascular', 'treatment_of_paget_disease', 'risk_of_cardiovascular_disease', 'treatment_of_cushing_disease', 'transposition_of_the_great_arteries', 'epidemics_of_respiratory_disease', 'patients_with_alcoholic_liver', 'exposure_to_infectious_bursal', 'complement_by_the_alternative', 'progression_of_pulmonary_vascular', 'transplants_by_the_nononcogenic', 'variants_of_marek_disease', 'development_of_chronic_liver', 'involved_in_the_pathogenesis', 'treatment_of_hodgkin_disease', 'susceptibility_to_the_disease', 'patients_with_multiple_sclerosis', 'classification_of_von_willebrand', 'antigen_and_beta_microglobulin', 'etiology_of_the_disease', 'months_after_the_onset', 'patients_had_connective_tissue', 'stage_iii_and_iv', 'found_to_have_raised_serum', 'infected_with_marek_disease', 'remain_alive_and_remain_alive', 'risk_for_cardiovascular_disease', 'cellular_immunity_to_protein', 'carried_out_to_establish', 'patients_with_respiratory_disease', 'case_of_whipple_disease', 'patients_with_beh_et', 'patients_with_liver_disease', 'correlated_with_the_degree', 'outbreak_of_legionnaires_disease', 'greater_than_or_equal', 'correlation_could_be_found', 'cells_in_the_bone_marrow', 'creation_of_ventricular_septal', 'short_and_long_term', 'patients_with_coronary_heart', 'sarcoidosis_and_whipple_disease', 'choline_in_alzheimer_disease', 'diagnosis_of_whipple_disease', 'levamisole_in_the_treatment', 'important_in_the_diagnosis', 'risk_of_coronary_heart', 'vaccine_against_marek_disease', 'onset_of_the_disease', 'clinical_course_of_the_disease', 'results_of_these_studies', 'smoking_and_coronary_heart', 'asia_foot_and_mouth', 'antigen_in_the_diagnosis', 'extent_of_the_disease', 'screening_test_for_coeliac_disease', 'paget_disease_of_the_vulva', 'patients_with_huntington_disease', 'patients_with_ischaemic_heart', 'foot_and_mouth_disease', 'aetiology_of_the_disease', 'patients_with_advanced_hodgkin', 'prenatal_diagnosis_of_genetic', 'patients_with_coeliac_disease', 'characteristic_of_whipple_disease', 'vp_of_foot_and_mouth', 'patients_with_chronic_liver', 'stages_of_the_disease', 'paget_disease_of_bone', 'patient_with_parkinson_disease'}\n"
     ]
    }
   ],
   "source": [
    "phrases = Phrases(bigram[matched_sents], common_terms=common_terms, min_count=1, threshold=1)\n",
    "trigram = Phraser(phrases)\n",
    "\n",
    "print_phrases(trigram, bigram[matched_sents], num_underscores=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOC NUMBER: 0\n",
      "\n",
      "ORIGINAL SENTENT: immune complexes in rheumatic disease\n",
      "\n",
      "BIGRAM: immune_complexes in rheumatic disease\n",
      "\n",
      "TRIGRAM: immune_complexes in rheumatic_disease\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc_num in [0]:\n",
    "    print('DOC NUMBER: {}\\n'.format(doc_num))\n",
    "    print('ORIGINAL SENTENT: {}\\n'.format(' '.join(matched_sents[doc_num])))\n",
    "    print('BIGRAM: {}\\n'.format(' '.join(bigram[matched_sents[doc_num]])))\n",
    "    print('TRIGRAM: {}'.format(' '.join(trigram[bigram[matched_sents[doc_num]]])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export Cleaned Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the cleaned text to a new file for later use\n",
    "with open(CLEANED_TEXT_PATH, 'w') as f:\n",
    "    for line in bigram[matched_sents]:\n",
    "        line = ' '.join(line) + '\\n'\n",
    "        line = line.encode('ascii', errors='ignore').decode('ascii')\n",
    "        f.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mlguild]",
   "language": "python",
   "name": "conda-env-mlguild-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
