{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing Exercise\n",
    "##### Author: Alex Sherman | alsherman@deloitte.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agenda\n",
    "\n",
    "1. SpaCy\n",
    "2. Text Tokenization, POS Tagging, Parsing, NER\n",
    "3. Text Pipelines\n",
    "4. Python Fundamentals: Collections, Itertools, list comprehensions, and Sorted\n",
    "5. Text Rule-based matching\n",
    "6. Advanced SpaCy Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from IPython.core.display import display, HTML\n",
    "from IPython.display import Image\n",
    "from configparser import ConfigParser, ExtendedInterpolation\n",
    "\n",
    "config = ConfigParser(interpolation=ExtendedInterpolation())\n",
    "config.read('../../config.ini')\n",
    "DB_PATH = config['DATABASES']['PROJECT_DB_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm DB_PATH is in the correct db directory, otherwise the rest of the code will not work\n",
    "DB_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to create a local database - commented out as the database is already provided\n",
    "\n",
    "# f = r'..\\raw_data\\pubmed\\pubmed_2M.txt'                                     # path to pubmed data (may differ on your machine)\n",
    "# engine = create_engine(DB_PATH)                                             # DB connection string\n",
    "# pd.read_csv(f, header=None, names=['text']).to_sql('pubmed', con=engine)    # read dataset and write to db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for the names of the tables in the database\n",
    "engine = create_engine(DB_PATH)\n",
    "pd.read_sql(\"SELECT name FROM sqlite_master WHERE type='table'\", con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the oracle 10k documents \n",
    "df = pd.read_sql(\n",
    "      \"SELECT * FROM pubmed\"\n",
    "    , index_col='index'\n",
    "    , con=engine\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase the number of characters displayed in each column\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example text\n",
    "text = df.text[27]\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy\n",
    "\n",
    "\"SpaCy is a free, open-source library for advanced Natural Language Processing (NLP) in Python.\n",
    "\n",
    "If you're working with a lot of text, you'll eventually want to know more about it. For example, what's it about? What do the words mean in context? Who is doing what to whom? What companies and products are mentioned? Which texts are similar to each other?\n",
    "\n",
    "SpaCy is designed specifically for production use and helps you build applications that process and \"understand\" large volumes of text. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning.\n",
    "\n",
    "SpaCy is not research software. It's built on the latest research, but it's designed to get things done. This leads to fairly different design decisions than NLTK or CoreNLP, which were created as platforms for teaching and research. The main difference is that SpaCy is integrated and opinionated. SpaCy tries to avoid asking the user to choose between multiple algorithms that deliver equivalent functionality. Keeping the menu small lets SpaCy deliver generally better performance and developer experience.\"\n",
    "\n",
    "### SpaCy Features \n",
    "\n",
    "NAME |\tDESCRIPTION |\n",
    ":----- |:------|\n",
    "Tokenization|Segmenting text into words, punctuations marks etc.|\n",
    "Part-of-speech (POS) Tagging|Assigning word types to tokens, like verb or noun.|\n",
    "Dependency Parsing|\tAssigning syntactic dependency labels, describing the relations between individual tokens, like subject or object.|\n",
    "Lemmatization|\tAssigning the base forms of words. For example, the lemma of \"was\" is \"be\", and the lemma of \"rats\" is \"rat\".|\n",
    "Sentence Boundary Detection (SBD)|\tFinding and segmenting individual sentences.|\n",
    "Named Entity Recognition (NER)|\tLabelling named \"real-world\" objects, like persons, companies or locations.|\n",
    "Similarity|\tComparing words, text spans and documents and how similar they are to each other.|\n",
    "Text Classification|\tAssigning categories or labels to a whole document, or parts of a document.|\n",
    "Rule-based Matching|\tFinding sequences of tokens based on their texts and linguistic annotations, similar to regular expressions.|\n",
    "Training|\tUpdating and improving a statistical model's predictions.|\n",
    "Serialization|\tSaving objects to files or byte strings.|\n",
    "\n",
    "SOURCE: https://spacy.io/usage/spacy-101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy Installation:\n",
    "- Windows: Download Microsoft Visual C++: \n",
    "1.\tGo to: https://www.visualstudio.com/downloads/#build-tools-for-visual-studio-2017\n",
    "2.\tDownload the first link for Visual Studio Community 2017\n",
    "3.\tDuring the install select the option to install Desktop with Development C++ (see image below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desktop with Development C++\n",
    "Image(\"../../raw_data/images/visual_studio_community.png\", width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SpaCy Installation\n",
    "Run the following using git bash as an administrator (i.e. right click on the git bash logo and select 'Run as Admin')\n",
    "- conda install -c conda-forge spacy\n",
    "- python -m spacy download en\n",
    "\n",
    "##### if you run into an error try the following:\n",
    "- python -m spacy link en_core_web_sm en\n",
    "- SOURCE: https://github.com/explosion/spaCy/issues/950\n",
    "\n",
    "##### Optional to install a convolutional neural network model  (~800MB). This is the model I will use in class:\n",
    "- python -m spacy download en_core_web_lg\n",
    "\n",
    "##### Test the following code from git bash (even if previous step failed):\n",
    "start python\n",
    "- python -i\n",
    "\n",
    "test if SpaCy was downloaded\n",
    "- import spacy\n",
    "\n",
    "approach 1: test if model downloaded\n",
    "- nlp = spacy.load('en') \n",
    "\n",
    "appraoch 2: test this if spacy.load('en') failed\n",
    "- import en_core_web_sm\n",
    "- nlp = en_core_web_sm.load()\n",
    "\n",
    "exit Python\n",
    "- exit()\n",
    "\n",
    "\n",
    "##### Optional - install on an AWS EC2 instance\n",
    "Instance: Amazon Linux 2 LTS Candidate 2 AMI (HVM), SSD Volume Type\n",
    "\n",
    "- #!/bin/bash\n",
    "- sudo yum update -y\n",
    "- sudo yum groupinstall 'Development Tools' -y\n",
    "- sudo easy_install pip\n",
    "- sudo yum install python-devel -y\n",
    "- sudo pip install spacy\n",
    "- sudo python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm which conda environment you are using - make sure it is one with SpaCy installed\n",
    "import sys\n",
    "sys.executable\n",
    "\n",
    "# if you have difficulty importing spacy try the following in git bash\n",
    "# conda install ipykernel --name Python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# read in a simple (small) English language model\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# another approach:\n",
    "# import en_core_web_sm\n",
    "# nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# read in a (large) convolutional neural network model\n",
    "# this will only work after the CNN model is downloaded (~800MB)\n",
    "# e.g. python -m spacy download en_core_web_lg\n",
    "nlp = spacy.load('en_core_web_lg') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the document text\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the text from the SpaCy object\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which the SpaCy document methods and attributes\n",
    "print(dir(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Pipeline\n",
    "\n",
    "When you read the text into spaCy, e.g. doc = nlp(text), you are applying a pipeline of nlp processes to the text.\n",
    "by default spaCy applies a tagger, parser, and ner, but you can choose to add, replace, or remove these steps.\n",
    "Note: Removing unnecessary steps for a given nlp can lead to substantial descreses in processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpaCy pipeline\n",
    "spacy_url = 'https://spacy.io/assets/img/pipeline.svg'\n",
    "iframe = '<iframe src={} width=1000 height=200></iframe>'.format(spacy_url)\n",
    "HTML(iframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "SpaCy first tokenizes the text, i.e. segments it into words, punctuation and so on. This is done by applying rules specific to each language. For example, punctuation at the end of a sentence should be split off – whereas \"U.K.\" should remain one token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokenization_url = 'https://spacy.io/assets/img/tokenization.svg'\n",
    "iframe = '<iframe src={} width=650 height=400></iframe>'.format(tokenization_url)\n",
    "HTML(iframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lexeme - entries in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a list of stop words from SpaCy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "print('Example stop words: {}'.format(list(STOP_WORDS)[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['have']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(nlp.vocab['have']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['have'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for word in the SpaCy vocabulary and\n",
    "# change the is_stop attribute to True (default is False)\n",
    "\n",
    "for word in STOP_WORDS:\n",
    "    nlp.vocab[word].is_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-speech (POS) Tagging\n",
    "\n",
    "After tokenization, spaCy can parse and tag a given Doc. This is where the statistical model comes in, which enables spaCy to make a prediction of which tag or label most likely applies in this context. A model consists of binary data and is produced by showing a system enough examples for it to make predictions that generalize across the language – for example, a word following \"the\" in English is most likely a noun.\n",
    "\n",
    "Annotation | Description\n",
    ":----- |:------|\n",
    "Text |The original word text|\n",
    "Lemma |The base form of the word.|\n",
    "POS |The simple part-of-speech tag.|\n",
    "Tag |The detailed part-of-speech tag.|\n",
    "Dep |Syntactic dependency, i.e. the relation between tokens.|\n",
    "Shape |The word shape – capitalisation, punctuation, digits.|\n",
    "Is Alpha |Is the token an alpha character?|\n",
    "Is Stop |Is the token part of a stop list, i.e. the most common words of the language?|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review document\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if POS tags were added to the doc in the NLP pipeline\n",
    "doc.is_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print column headers\n",
    "print('{:15} | {:15} | {:8} | {:8} | {:11} | {:8} | {:8} | {:8} | '.format(\n",
    "    'TEXT','LEMMA_','POS_','TAG_','DEP_','SHAPE_','IS_ALPHA','IS_STOP'))\n",
    "\n",
    "# print various SpaCy POS attributes\n",
    "for token in doc[0:20]:\n",
    "    print('{:15} | {:15} | {:8} | {:8} | {:11} | {:8} | {:8} | {:8} |'.format(\n",
    "          token.text, token.lemma_, token.pos_, token.tag_, token.dep_\n",
    "        , token.shape_, token.is_alpha, token.is_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Dependency Parsing\n",
    "\n",
    "spaCy features a fast and accurate syntactic dependency parser, and has a rich API for navigating the tree. The parser also powers the sentence boundary detection, and lets you iterate over base noun phrases, or \"chunks\". You can check whether a Doc  object has been parsed with the doc.is_parsed attribute, which returns a boolean value. If this attribute is False, the default sentence iterator will raise an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check is document has been parsed (dependency parsing)\n",
    "doc.is_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:15} | {:10} | {:15} | {:10} | {:25} | {:25}'.format(\n",
    "    'TEXT','DEP','HEAD TEXT','HEAD POS','CHILDREN','LEFTS'))\n",
    "\n",
    "for token in doc[0:20]:\n",
    "    print('{:15} | {:10} | {:15} | {:10} | {:25} | {:25}'.format(\n",
    "        token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "        str([child for child in token.children]), str([t.text for t in token.lefts])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOUN CHUNCKS:\n",
    "\n",
    "| **TERM** | Definition |\n",
    "|:---|:---:|\n",
    "| **Text** | The original noun chunk text |\n",
    "| **Root text** | The original text of the word connecting the noun chunk to the rest of the parse |\n",
    "| **Root dependency** | Dependency relation connecting the root to its head |\n",
    "| **Root head text** | The text of the root token's head |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:15} | {:10} | {:15} | {:40}'.format('ROOT_TEXT','ROOT','DEPENDENCY','TEXT'))\n",
    "\n",
    "for chunk in list(doc.noun_chunks)[0:20]:\n",
    "    print('{:15} | {:10} | {:15} | {:40}'.format(\n",
    "        chunk.root.text, chunk.root.dep_, chunk.root.head.text, chunk.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependency visualization\n",
    "\n",
    "# show visualization in Jupyter Notebook\n",
    "displacy.render(docs=doc, style='dep', jupyter=True)\n",
    "\n",
    "# Another Option\n",
    "# uncomment and run the below code, then open another browser tab and go to http://localhost:5000\n",
    "# when you are done (before you run the next cell in the notebook) stop this cell\n",
    "# displacy.serve(docs=doc, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER)\n",
    "\n",
    "A named entity is a \"real-world object\" that's assigned a name – for example, a person, a country, a product, or a book title. spaCy can recognise various types of named entities in a document, by asking the model for a prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_text = \"When I told John that I wanted to move to Alaska, he warned me that I'd have trouble finding a Starbucks there.\"\n",
    "ner_doc = nlp(ner_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:10} | {:15}'.format('LABEL','ENTITY'))\n",
    "\n",
    "for ent in ner_doc.ents[0:20]:\n",
    "    print('{:10} | {:50}'.format(ent.label_, ent.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ent methods and attributes\n",
    "print(dir(ent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity visualization\n",
    "# after you run this code, open another browser and go to http://localhost:5000\n",
    "# when you are done (before you run the next cell in the notebook) stop this cell \n",
    "\n",
    "displacy.render(docs=ner_doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "If you have a sequence of documents to process, you should use the Language.pipe()  method. The method takes an iterator of texts, and accumulates an internal buffer, which it works on in parallel. It then yields the documents in order, one-by-one.\n",
    "\n",
    "- batch_size: number of docs to process per thread\n",
    "- n_threads: number threads to use (-1 is the default that let's SpaCy decide)\n",
    "- disable: Names of pipeline components to disable to speed up text processing.\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.pipeline import Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with a subset of the data, mentioning the word immune\n",
    "immune_df = df[df.text.str.contains('immune')].text\n",
    "\n",
    "# print the count of matches\n",
    "print('Lines with the term immune: {}\\n'.format(len(immune_df)))\n",
    "\n",
    "# view the first five section names\n",
    "for line in immune_df.head(2):\n",
    "    print(line)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for doc in nlp.pipe(immune_df.head(10)): # includes ['parser','tagger','ner']\n",
    "    if 'immune' in doc.text:\n",
    "        print(doc)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy - Tips for faster processing\n",
    "\n",
    "You can substantially speed up the time it takes SpaCy to read a document by disabling components of the NLP that are not necessary for a given task.\n",
    "\n",
    "- Disable options: **parser, tagger, ner**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# processing occurs ~75x faster by disabling pipeline components\n",
    "for doc in nlp.pipe(immune_df.head(10), disable=['parser','tagger','ner']):\n",
    "    if 'immune' in doc.text:\n",
    "        print(doc)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "1. print all the distinct entities tagged with 'CARDINAL'\n",
    "2. print all the distinct entities tagged with 'PERSON'\n",
    "3. print all the distinct entities tagged with 'GPE'\n",
    "\n",
    "For all exercises:\n",
    "- use a batch size of 100\n",
    "- disable the parser and tagger (ner is needed to add the tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# print all the distinct entities tagged as a CARDINAL\n",
    "# search in immune_df.head(200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# print all the distinct entities tagged as an organization (ORG)\n",
    "# search in immune_df.head(500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# print all the distinct entities tagged as a geopolitical entity (GPE)\n",
    "# search in immune_df.head(1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Collections - DefaultDict\n",
    "\n",
    "Usually, a Python dictionary throws a KeyError if you try to get an item with a key that is not currently in the dictionary. The defaultdict in contrast will simply create any items that you try to access (provided of course they do not exist yet). To create such a \"default\" item, it calls the function object that you pass in the constructor (more precisely, it's an arbitrary \"callable\" object, which includes function and type objects). For the first example, default items are created using int(), which will return the integer object 0. For the second example, default items are created using list(), which returns a new empty list object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed_sentence = \"\"\"PubMed Description: \n",
    "PubMed comprises more than 28 million citations for biomedical literature from MEDLINE, life science journals, and online books.\n",
    "Citations may include links to full-text content from PubMed Central and publisher web sites.\"\"\".strip()\n",
    "\n",
    "example_doc = nlp(pubmed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG APPROACH - KeyError!\n",
    "\n",
    "# try to create a word count dict with new keys\n",
    "d = {}\n",
    "for word in example_doc:\n",
    "    d[word] += 1  # cannot add if the key does not exist\n",
    "\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = defaultdict(int)  # define the type of data the dict stores\n",
    "for word in example_doc:\n",
    "    d[word.text] += 1  # can add to unassigned keys\n",
    "\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Collections - Counter\n",
    "\n",
    "A Counter is a dict subclass for counting hashable objects. It is an unordered collection where elements are stored as dictionary keys and their counts are stored as dictionary values. Counts are allowed to be any integer value including zero or negative counts. The Counter class is similar to bags or multisets in other languages.\n",
    "\n",
    "SOURCE: https://docs.python.org/2/library/collections.html#collections.Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of times each CARDINAL appears\n",
    "print(Counter(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Iterrtools - combinations\n",
    "\n",
    "\"The itertools module standardizes a core set of fast, memory efficient tools that are useful by themselves or in combination. Together, they form an “iterator algebra” making it possible to construct specialized tools succinctly and efficiently in pure Python.\n",
    "\n",
    "**Combinations**\n",
    "- Return r length subsequences of elements from the input iterable.\n",
    "- Combinations are emitted in lexicographic sort order. So, if the input iterable is sorted, the combination tuples will be produced in sorted order.\n",
    "- Elements are treated as unique based on their position, not on their value. So if the input elements are unique, there will be no repeat values in each combination.\n",
    "\n",
    "SOURCE: https://docs.python.org/3.4/library/itertools.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = ['PubMed','Medline','Citation','Biomedical']\n",
    "for combo in combinations(terms, 2):\n",
    "    print(combo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### List Comprehensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traditional iteration\n",
    "\n",
    "terms_subset = []\n",
    "for term in terms:\n",
    "    if 'med' in term.lower():\n",
    "        terms_subset.append(term)\n",
    "\n",
    "terms_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list comprehension\n",
    "\n",
    "#   return value   iteration           conditional\n",
    "#[  term           for term in terms   if 'med' in term.lower()]\n",
    "\n",
    "[term for term in terms if 'med' in term.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sorted\n",
    "\n",
    "sorted(iterable, key=None, reverse=False)\n",
    "\n",
    "- Return a new sorted list from the items in iterable.\n",
    "- Has two optional arguments which must be specified as keyword arguments.\n",
    "- key specifies a function of one argument that is used to extract a comparison key from each list element: key=str.lower. The default value is None (compare the elements directly).\n",
    "- reverse is a boolean value. If set to True, then the list elements are sorted as if each comparison were reversed.\n",
    "\n",
    "SOURCE: https://docs.python.org/3/library/functions.html#sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles =[('article2', 3),('article3', 2),('article1', 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(articles, key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(articles, key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort based on the last character of the first term\n",
    "sorted(articles, key=lambda x:x[0][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "1. Count how many time each individual entity appears\n",
    "2. Create a mapping that keeps track of every combination of entities pairs that appear in the same sentence\n",
    "3. Count how many times each entity combo appears\n",
    "4. Print the entity combos (using sorted) in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# create a defaultdict(int) called entity_relations\n",
    "entity_relations = \n",
    "\n",
    "# create an empty list called counter_entities \n",
    "counter_entities = \n",
    "\n",
    "# during testing set .head() to a smaller number such as .head(1000) \n",
    "for doc in nlp.pipe(immune_df.head(10000), disable=['parser','tagger', 'ner']):\n",
    "\n",
    "    # store the token.text for all the tokens containing the letters 'toxic' (i.e. 'toxic' in term)\n",
    "    # use a list comprehension\n",
    "    entities = \n",
    "\n",
    "    # add the tokens from the current doc to counter_entities (use += to add the token.text)\n",
    "    counter_entities \n",
    "    \n",
    "    # create combinations of two terms each time multiple 'toxic' words appear\n",
    "    # increment the count in entity_relations defaultdict each time a combo is repeated\n",
    "    for combo in combinations(entities, 2):\n",
    "        entity_relations[combo] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Counter(counter_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the entity pairs in descending order\n",
    "sorted(entity_relations.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Identify Relevant Text (Rule-based Matching)\n",
    "\n",
    "Finding sequences of tokens based on their texts and linguistic annotations, similar to regular expressions. We will use this to filter and extract relevant text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_basesd_matching_url = 'https://spacy.io/usage/linguistic-features#rule-based-matching'\n",
    "iframe = '<iframe src={} width=1000 height=700></iframe>'.format(rule_basesd_matching_url)\n",
    "HTML(iframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Matcher identifies text from rules we specify\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to specify what to do with the matching text\n",
    "\n",
    "def collect_sents(matcher, doc, i, matches):\n",
    "    \"\"\"  collect and transform matching text\n",
    "\n",
    "    :param matcher: Matcher object\n",
    "    :param doc: is the full document to search for text patterns\n",
    "    :param i: is the index of the text matches\n",
    "    :param matches: matches found in the text\n",
    "    \"\"\"\n",
    "    \n",
    "    match_id, start, end = matches[i]  # indices of matched term\n",
    "    span = doc[start:end]              # extract matched term\n",
    "    \n",
    "    print('span: {} | start_ind:{:5} | end_ind:{:5} | id:{}'.format(\n",
    "        span, start, end, match_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# set a pattern of text to collect\n",
    "# find all mentions of the word fees \n",
    "pattern = [{'LOWER':'disease'}] # LOWER coverts words to lowercase before matching\n",
    "\n",
    "# instantiate matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# add pattern to the matcher (one matcher can look for many unique patterns)\n",
    "# provice a pattern name, function to apply to matches, pattern to identify\n",
    "matcher.add('disease', collect_sents, pattern)\n",
    "\n",
    "# pass the doc to the matcher to run the collect_sents function\n",
    "for doc in nlp.pipe(immune_df.head(100), disable=['parser','tagger','ner']): \n",
    "    matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the function to print the sentence of the matched term (span)\n",
    "\n",
    "def collect_sents(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "    print('SPAN: {}'.format(span))\n",
    "\n",
    "    # span.sent provides the sentence that contains the span\n",
    "    print('SENT: {}'.format(span.sent))\n",
    "    print()\n",
    "\n",
    "\n",
    "# update the pattern to look for any noun preceeding the term 'fees'\n",
    "pattern = [{'POS': 'NOUN', 'OP': '+'},{'LOWER':'disease'}]\n",
    "matcher = Matcher(nlp.vocab)  # reinstantiate the matcher to remove previous patterns\n",
    "matcher.add('disease', collect_sents, pattern)\n",
    "\n",
    "for doc in nlp.pipe(immune_df.head(100), disable=['parser','ner']): # enable pos tagger\n",
    "    matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the function to collect sentences\n",
    "\n",
    "def collect_sents(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "        \n",
    "    # update matched data collections\n",
    "    matched_sents.append(span.sent)\n",
    "\n",
    "\n",
    "matched_sents = []  # container for sentences\n",
    "pattern = [{'POS': 'NOUN', 'OP': '+'},{'LOWER':'disease'}]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('disease', collect_sents, pattern)\n",
    "\n",
    "for doc in nlp.pipe(immune_df.head(100), disable=['ner']): # enable parser to collect sents\n",
    "    matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review matches\n",
    "set(matched_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the function to count matches using defaultdict\n",
    "\n",
    "def collect_sents(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "    \n",
    "    # update matched data collections\n",
    "    ent_count[span.text] += 1  # defaultdict keys must use span.text not span!\n",
    "\n",
    "\n",
    "ent_count = defaultdict(int)\n",
    "pattern = [{'LOWER':'disease'}]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('disease', collect_sents, pattern)\n",
    "\n",
    "for doc in nlp.pipe(immune_df.head(100), disable=['pos','ner']): # enable parser to collect sents\n",
    "    matcher(doc)\n",
    "\n",
    "ent_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# update the pattern to look for a noun describing the fee\n",
    "\n",
    "ent_count = defaultdict(int)\n",
    "\n",
    "# change OP to 1 to only get a single term to the left\n",
    "pattern = [{'POS': 'NOUN', 'OP': '1'},{'LOWER':'disease'}]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('disease', collect_sents, pattern)\n",
    "\n",
    "for doc in nlp.pipe(immune_df.head(1000), disable=['ner']): # enable parser to collect sents\n",
    "    matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced SpaCy (Bonus Material)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Matching: Avoid Multiple Term Matches\n",
    "\n",
    "When using rule-based matching, SpaCy may match the same term multiple times if it is part of different n-term pairs with one term contained in another. For instance, 'integration services' in 'system integration services.'\n",
    "\n",
    "To avoid matching these terms multiple times, we can add to the collect_sents function to check if each term is contained in the previous term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_sents(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "    sent = span.sent\n",
    "    \n",
    "    # lemmatize the matched spans\n",
    "    entity = span.lemma_.lower()\n",
    "            \n",
    "    # explicity add the first entity without checking if it matches other terms\n",
    "    # as there is no previous span to check    \n",
    "    if i == 0:\n",
    "        ent_count[entity] += 1\n",
    "        ent_sents[entity].append(sent)\n",
    "        matched_sents.append(sent)\n",
    "        return\n",
    "\n",
    "    # get the span, entity, and sentence from the previous match\n",
    "    # if more than one match exist\n",
    "    last_match_id, last_start, last_end = matches[i-1]\n",
    "    last_span = doc[last_start : last_end]\n",
    "    last_entity = last_span.text.lower()\n",
    "    last_sent = last_span.sent\n",
    "\n",
    "    # to avoid adding duplicates when one term is contained in another \n",
    "    # (e.g. 'integration services' in 'system integration services')\n",
    "    # make sure new spans are unique\n",
    "    distinct_entity = (entity not in last_entity) or (sent != last_sent)\n",
    "    not_duplicate_entity = (entity != last_entity) or (sent != last_sent)\n",
    "    \n",
    "    # update collections for unique data\n",
    "    if distinct_entity and not_duplicate_entity:\n",
    "        ent_count[entity] += 1\n",
    "        ent_sents[entity].append(sent)\n",
    "        matched_sents.append(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multiple Patterns\n",
    "\n",
    "SpaCy matchers can use multiple patterns. Each pattern can be added to the Matcher individually with match.add and can use their own collect_sents function. Or use *patterns to add multiple patterns to the matcher at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sents = []\n",
    "ent_sents  = defaultdict(list)\n",
    "ent_count = defaultdict(int)\n",
    "\n",
    "# multiple patterns\n",
    "pattern = [[{'POS': 'NOUN', 'OP': '+'},{'LOWER': 'disease'}]\n",
    "           , [{'POS': 'NOUN', 'OP': '+'},{'LOWER': 'disorder'}]]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# *patterns to add multiple patterns with the same collect_sents function\n",
    "matcher.add('disease_disorder', collect_sents, *pattern)\n",
    "\n",
    "for doc in nlp.pipe(immune_df.head(500), disable=['ner']):\n",
    "    matches = matcher(doc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subject Verb Object (S,V,O) Extraction\n",
    "\n",
    "SOURCES: \n",
    "- http://textacy.readthedocs.io/en/latest/_modules/textacy/extract.html#subject_verb_object_triples\n",
    "- http://textacy.readthedocs.io/en/latest/_modules/textacy/spacy_utils.html#get_main_verbs_of_sent\n",
    "- https://github.com/chartbeat-labs/textacy/blob/master/textacy/constants.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nanmin, nanmax, zeros, NaN\n",
    "from itertools import takewhile\n",
    "from spacy.parts_of_speech import CONJ, DET, NOUN, VERB\n",
    "from spacy.tokens.span import Span as SpacySpan\n",
    "\n",
    "NUMERIC_NE_TYPES = {'ORDINAL', 'CARDINAL', 'MONEY', 'QUANTITY', 'PERCENT', 'TIME', 'DATE'}\n",
    "SUBJ_DEPS = {'agent', 'csubj', 'csubjpass', 'expl', 'nsubj', 'nsubjpass'}\n",
    "OBJ_DEPS = {'attr', 'dobj', 'dative', 'oprd'}\n",
    "AUX_DEPS = {'aux', 'auxpass', 'neg'}\n",
    "\n",
    "def subject_verb_object_triples(doc):\n",
    "    \"\"\"\n",
    "    Extract an ordered sequence of subject-verb-object (SVO) triples from a\n",
    "    spacy-parsed doc. Note that this only works for SVO languages.\n",
    "\n",
    "    Args:\n",
    "        doc (``textacy.Doc`` or ``spacy.Doc`` or ``spacy.Span``)\n",
    "\n",
    "    Yields:\n",
    "        Tuple[``spacy.Span``, ``spacy.Span``, ``spacy.Span``]: the next 3-tuple\n",
    "            of spans from ``doc`` representing a (subject, verb, object) triple,\n",
    "            in order of appearance\n",
    "    \"\"\"\n",
    "    # TODO: What to do about questions, where it may be VSO instead of SVO?\n",
    "    # TODO: What about non-adjacent verb negations?\n",
    "    # TODO: What about object (noun) negations?\n",
    "    if isinstance(doc, SpacySpan):\n",
    "        sents = [doc]\n",
    "    else:  # textacy.Doc or spacy.Doc\n",
    "        sents = doc.sents\n",
    "\n",
    "    for sent in sents:\n",
    "        start_i = sent[0].i\n",
    "\n",
    "        verbs = get_main_verbs_of_sent(sent)\n",
    "        for verb in verbs:\n",
    "            subjs = get_subjects_of_verb(verb)\n",
    "            if not subjs:\n",
    "                continue\n",
    "            objs = get_objects_of_verb(verb)\n",
    "            if not objs:\n",
    "                continue\n",
    "\n",
    "            # add adjacent auxiliaries to verbs, for context\n",
    "            # and add compounds to compound nouns\n",
    "            verb_span = get_span_for_verb_auxiliaries(verb)\n",
    "            verb = sent[verb_span[0] - start_i: verb_span[1] - start_i + 1]\n",
    "            for subj in subjs:\n",
    "                subj = sent[get_span_for_compound_noun(subj)[0] - start_i: subj.i - start_i + 1]\n",
    "                for obj in objs:\n",
    "                    if obj.pos == NOUN:\n",
    "                        span = get_span_for_compound_noun(obj)\n",
    "                    elif obj.pos == VERB:\n",
    "                        span = get_span_for_verb_auxiliaries(obj)\n",
    "                    else:\n",
    "                        span = (obj.i, obj.i)\n",
    "                    obj = sent[span[0] - start_i: span[1] - start_i + 1]\n",
    "\n",
    "                    yield (subj, verb, obj)\n",
    "\n",
    "def get_main_verbs_of_sent(sent):\n",
    "    \"\"\"Return the main (non-auxiliary) verbs in a sentence.\"\"\"\n",
    "    return [tok for tok in sent\n",
    "            if tok.pos == VERB and tok.dep_ not in {'aux', 'auxpass'}]\n",
    "\n",
    "def get_subjects_of_verb(verb):\n",
    "    \"\"\"Return all subjects of a verb according to the dependency parse.\"\"\"\n",
    "    subjs = [tok for tok in verb.lefts\n",
    "             if tok.dep_ in SUBJ_DEPS]\n",
    "    # get additional conjunct subjects\n",
    "    subjs.extend(tok for subj in subjs for tok in _get_conjuncts(subj))\n",
    "    return subjs\n",
    "\n",
    "def get_objects_of_verb(verb):\n",
    "    \"\"\"\n",
    "    Return all objects of a verb according to the dependency parse,\n",
    "    including open clausal complements.\n",
    "    \"\"\"\n",
    "    objs = [tok for tok in verb.rights\n",
    "            if tok.dep_ in OBJ_DEPS]\n",
    "    # get open clausal complements (xcomp)\n",
    "    objs.extend(tok for tok in verb.rights\n",
    "                if tok.dep_ == 'xcomp')\n",
    "    # get additional conjunct objects\n",
    "    objs.extend(tok for obj in objs for tok in _get_conjuncts(obj))\n",
    "    return objs\n",
    "\n",
    "def _get_conjuncts(tok):\n",
    "    \"\"\"\n",
    "    Return conjunct dependents of the leftmost conjunct in a coordinated phrase,\n",
    "    e.g. \"Burton, [Dan], and [Josh] ...\".\n",
    "    \"\"\"\n",
    "    return [right for right in tok.rights\n",
    "            if right.dep_ == 'conj']\n",
    "\n",
    "def get_span_for_verb_auxiliaries(verb):\n",
    "    \"\"\"\n",
    "    Return document indexes spanning all (adjacent) tokens\n",
    "    around a verb that are auxiliary verbs or negations.\n",
    "    \"\"\"\n",
    "    min_i = verb.i - sum(1 for _ in takewhile(lambda x: x.dep_ in AUX_DEPS,\n",
    "                                              reversed(list(verb.lefts))))\n",
    "    max_i = verb.i + sum(1 for _ in takewhile(lambda x: x.dep_ in AUX_DEPS,\n",
    "                                              verb.rights))\n",
    "    return (min_i, max_i)\n",
    "\n",
    "def get_span_for_compound_noun(noun):\n",
    "    \"\"\"\n",
    "    Return document indexes spanning all (adjacent) tokens\n",
    "    in a compound noun.\n",
    "    \"\"\"\n",
    "    min_i = noun.i - sum(1 for _ in takewhile(lambda x: x.dep_ == 'compound',\n",
    "                                              reversed(list(noun.lefts))))\n",
    "    return (min_i, noun.i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in nlp.pipe(immune_df.head(50), disable=['pos','ner']): # enable parser to collect sents\n",
    "    \n",
    "    # collect triples\n",
    "    triples = [(s,v,o) for s,v,o in subject_verb_object_triples(doc)]\n",
    "    \n",
    "    if triples:\n",
    "        print(triples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
